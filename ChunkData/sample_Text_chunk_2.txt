stems januaryjune ajczxye n sr ajzx c uzdeczsfezx z acze c vvteczt wcd hzeyfe hczeev avczddz w sr prohibited required complex cluster algorithms eyvc ucrhsrt w eyv rxczey zd eyre efficient methods defining initial number partitions many alternative methods improve kvrd hvcv afszdyvu z literature krishna murty proposed r vh vvezt kmeans algorithm gka global search faster convergence zhang mzx br ru df acadvu arcrv kmeans algorithm higher efficiency many algorithms similar kmeans ap peared literature duda hart stork brtffvv fuzzy cmeans fcm clustering kmeans algorithm sample assigned one cluster fuzzy clustering cvrivd eyzd cvdecztez ru r svte tr svx e dvgvcr tfdevcd eyv drv ezv sfe hzey tvc erz uvxcvvd w vsvcdyzad iyv de h fuzzy clustering method fuzzy cmeans method fcm introduced dunn later generalized bezdek fcm partitions data set x x x x r n p p features c wfkkj dfsdved hyvcv uik membership ak class ic classes identified cluster centers vi ic iyv svtezgv w b zd find optimal fuzzy c partition minimizing eyv svtezgv wftez jmuv x u x v ik c k n k hyvcv eyv grfv w wfkkj arcezez reczi u tdecrzvu z eyv crxv pq dfty eyre u k n ij c u n c ik k n zd r viavezr hvzxye ing function controls fuzziness membership values euclidean norm v v v vc zd r reczi w fh cluster centers v r p ic fuzzy c means algorithm minimize step choose appropriate values c small positive number initial ize randomly fuzzy partition matrix u set iteration number step given membership values uik calculate cluster centers vi ic v u x u k n ik k k n ik step zgv r vh tfdevc tvevc wc step update membership values uik using uik x v u k k n ik j c step repeat step u ut predefined number iterations reached methods discussed previous sections rcv tczdayrcu arcezezzx veyud hyzty r h e arcezez urer ze r davtzwzvu fsvc w fefrj vitfdzgv urerdved j hyzv wfkkj veyud rcv dwe arcezezzx veyud hyvcv r svte tr svx e v c cv urer dved partitions similarly crisphard parti tioning methods selection initial matrix centers plays important role convergence fcm many times fcm guarantee global optimal solutions due randomized initialization cluster centers matrix u moreover fcm solutions also sensitive zdv ru fezvcd reyhrj vkuv ru f proposed modified fcm using international journal agricultural environmental information systems januaryjune norm distance increase robustness outliers hung yangs algorithm hung yang finds actual clusters centers refines initial value fcm technique reduces computational time large amount many improved fcms applications also found hammah fccr reyrhrj vkuv rcrjzrzd oyrx r knearest neighbor classification knearest neighborhood mitchell veyu zd hzuvj ruaevu ufv e zed vwwztzvtj key idea algorithm classify vh drav z eyv de wcvbfve trdd w zed closest neighbors training set rczej gezx wcfr eyv trdd rsvd w neighbors euclidean distance measure zd fdvu e trtfrev yh tdv vrty vsvc w training set target data examined knearest neighbor classification rxczey tr sv uzgzuvu ze eh ayrdvd training phase vsv r ecrzzx x x x n n hyvcv x x x x dve ddimen sional feature vector real numbers vsv trdd rsvd correspond ing xi c hyvcv c n different classes task determine nc number ynew cnew testing phase find closest point xj xnew hzey cv spect euclidean distance x x x x j new j classify new new j dvczfd ucrhsrt w eyzd knearest neighbor technique computational complexity searching k nearest neighbors among available training samples kuncheva claims achieve better computational efficiency higher classification accuracy using genetic algorithms editing techniques vcv ru rsvderj acadvu r cc rxczey hzey tr erckv hzuh vdezrev e improve approximation quality also sug xvdevu r ruraezgv vrczx rxczey e rh wvhvc urer azed e sv fdvu z r ecrzzx urer set many techniques proposed reduce computational burden knearest vzxysc rxczeyd z hrx ru lv pan qiao sun artificial neural networks cezwztzr vfcr vehcd cc rcv vwwztzve classification tools applied several applications including extracting regularities data classifying events finance marketing internet biomedicine training process uses available examples e acuftv r uv ru e trddzwj vh vgved srdvu eyv viecrtevu uv cvfcr vehcd built simple units called neurons tvd sj rrxj hzey eyv hrj yfr scrz hcd cvfcd rcv zvu hzey vrty eyvc sj r dve w hvzxyevu tvtezd iyv zwcrez analyzed fed neurons input layer propagated neurons hidden layers processing result processing propagated next hidden layer process con tinued output layer reached unit receives information units ru actvddvd eyzd zwcrez hyzty hz sv converted output unit ajczxye n sr ajzx c uzdeczsfezx z acze c vvteczt wcd hzeyfe hczeev avczddz w sr prohibited international journal agricultural environmental information systems januaryjune ajczxye n sr ajzx c uzdeczsfezx z acze c vvteczt wcd hzeyfe hczeev avczddz w sr prohibited davtzwzt veyud w tydzx eyv vehc parameters number hidden layers type activation function generally one input one output nodes chosen data class iyv ecrzzx dve zd h r aczcz ru zd fdvu e wzvefv eyv vehc wc wfefcv dzzrc cvtcud lyzv z ecrzzx ayrdv h urer containing inputs corresponding outputs wvu e eyv vehc ru eyv vehc vrcd e zwvc eyv cvrezdyza svehvv eyvdv eh process classification ann sv scruj uvwzvu rd whd run sample training set giving attribute values input iyv dfrez w hvzxyed ru rtezgrez functions applied node hid den output layers output xvvcrevu wvvuwchrcu actvdd arcv feafe hzey eyv viavtevu feafe training set output match go back layer rjvc ru uzwj rct hvzxyed ru szrdvd nodes backpropagation process run next sample process gvefrj eyv hvzxyed hz tgvcxv ru process stops vvu wchrcu eaxj zd hzuvj fdvu z fezrjvc avctvaezd vehcd vvu wchrcu vehc acgzuvd r xvvcr wcrvhc wc representing nonlinear functional mappings svehvv r dve w zafe grczrsvd ru r dve w feafe grczrsvd zdya vh zd r sczvw gvcgzvh w wvvu wchrcu vehc wc training sample x hidden output layer node j calculate input ij node wx w j ji j calculate output oj node e j j rtacarxrez zd hzuvj fdvu rxczey wc eyv afcadv w ecrzzx eyv vfcr vehcd backpropagation algorithm considered rd r ehdeva actvdd eyv wzcde deva eyv uvczgrezgvd w eyv vccc wftez hzey cvdavte e eyv hvzxyed rcv vgrfrevu dvtu deva derivatives used compute rufdeved e sv ruv e eyv hvzxyed sj fd ing gradient descent optimization dtyvvd gvcgzvh w srtacarxrez tr sv xzgv rd svh node j output layer calculate error err j j j j node j hidden layer calculate error err errw j j j k jk k c vrty hvzxye wij trtfrev hvzxye z crement w l err ij j ch faurev eyv acvgzfd hvzxye rd w w w ij ij ij bias calculate bias increment j j l err zrj faurev szrd hzey j j j international journal agricultural environmental information systems januaryjune ajczxye n sr ajzx c uzdeczsfezx z acze c vvteczt wcd hzeyfe hczeev avczddz w sr prohibited cvfcr vehcd rcv hzuvj fdvu z trd sification still many unsolved zddfvd z raajzx vfcr vehcd dfty rd scalability misclassification convergence higher mean square errors forth many researchers tried overcome issues ru acadvu grczvej w vfcr vehcd hzey better performances jiang harvey wah acadvu r vh raacrty w tdecfte zx ru ecrzzx vfcr vehcd e gvctv problems including local minima dh tgvcxvtv w eyv vrczx actvdd eyvzc raacrty wvvuwchrcu vehc hrd constructed based data clusters generated based locally trained clustering ltc trained using standard algorithms avcrezx eyv xsr ecrzzx dve hyzty tgvcxvd crazuj ufv e zed zyvczevu h vuxv hzey xu xvvcrzkrez rszzej wc eyv global training ji proposed vrczx veyu srdvu tszrez w hvr trddzwzvcd hyzty hvcv wfu sj r cruzkvu algorithm achieve good generalization fast training time test problems ru eyv cvr raaztrezd iyvj dyhvu eyre zw eyv hvrvdd wrtec hrd tydv rttcuzx critical value given theory tszrezd w hvr trddzwzvcd tfu rtyzvgv r xu xvvcrzkrez avcwcrtv hzey aj nomial space timecomplexity yu chen cheng proposed dynamic learning using derivative information instead fixed learning rate optimize back propagation iyv zwcrez xreyvcvu wc eyv wchrcu ru srthrcu acarxrez hrd fdvu wc ujrzt vrczx hzey eyzd evtyzbfv eyvj rtyzvgvu higher convergence rate significant reduc tion learning process probability misclassification random sample termed generalization error classifier many researchers used ensemble methods reduce misclassification generalization vcccd rdv hrr rdyv schmeiser information clas sification errors learning generalization recent developments neural vehcd tr sv wfu z frcz afxdz ru venkatesh solazzia uncinib zhang recent studies data mining techniques wu et al consider ann top data mining technique support vector machines support vector machine svm state art machine learning algorithm cortes kraz kraz iyv rz zuvr w hkb zd e dvarcrev eyv zafe dartv z eh half spaces using hyperplane xw hyzty rizzkvd eyv rcxz svehvv eyv eh classes l eyvc hcud xzgv azed x r corresponding labels yi ifx b ifx hv vvu e wzu eh arcrv yjavcarvd xw hyzty dvarcrevd eyv azed svehvv eh trddvd iyv rcxz svehvv eh trddvd zd cvacvdvevu sj eyv uzdertv svehvv eyv eh yjavcarvd w thus optimization problem minimize norm w hzey tdecrzed e tccvtej classified points classes min w st x w n vcv hv ev eyre zzzkzx w minizing w iyzd veyu hcd hyv eyvcv zd r avcwvte zvrc dvarcrez svehvv eyv eh trddvd iyvcvwcv ze zd trvu eyv yrcu rcxz trddzwzvc lyv eyv eh trddvd rcv e linearly separable soft margin classifier used classifier finds hyperplane rhd r wvh azed e gzrev eyv dvarcrez number points correctly classified minimized case international journal agricultural environmental information systems januaryjune ajczxye n sr ajzx c uzdeczsfezx z acze c vvteczt wcd hzeyfe hczeev avczddz w sr prohibited optimization problem reduced eyv whzx min w c sty x w n n problems belong quadratic convex programming problems solved using standard quadratic programming techniques bertsekas nonlinear trddzwztrez eyv hkb zd fdvu hzey vcv wftezd brxrdrczr lzu ru basic solution technique linear programming generalized eigenvalue classification generalized eigenvalue classification methods proposed mangasari