signstospeech mobile app translating sign language speech using convolutional neural network thesis presented faculty college computing studies information communication technology isabela state university cauayan campus partial fulfillment requirements degree bachelor science computer science jenalyn l aglugub angelica mae p labuguen xylee manermanibog january abstract becomes difficult converse deaf mute people need language structure understand trying say language barrier normal people mute people aim create interface converts sign language text everyone understand sign language people unable speak purpose train model recognize sign language aim create mobile application detect sign language convert text voice researchers trained stateoftheart convolutional neural network cnn architecture shufflenetv adam sgd used optimization algorithms compare one would achieve highest accuracy training models researchers trained models detect sign language ie american sign language alphabet recognition american sign language adam optimization achieved highest accuracy average sgd achieved average accuracy accuracy adam sgd models demonstrates used detect sign language models chosen used mobile application handsign language translation model proved ideal mobile applications small size mb introduction background study recent years use different types controls besides mouse keyboard become common number devices available specific tasks applications one interaction form gained popularity area natural interactions humans machines web browsers video games virtual reality vr environments diverse set tools taken advantage users natural interactions eg voice control touchpads haptic devices cameras etc immersing user system environment natural way goal type research natural interaction involves use users body without additional hardware called natural user interface nui using sensors capturing diverse interactions users body possible recognize commands perform required tasks system hand gestures important part nonverbal communication form integral part interactions environment notably sign language set hand gestures valuable millions disabled people however deaf dumb users experience difficulty communicating outside world neither understand use sign language gesture recognition classification platforms aid translating gestures understand sign language yang et al furthermore hand gesture recognition help monitoring stroke rheumatoid arthritis patients watson healthcare professionals remotely monitor performance several patients using gesture classification system lower cost shorter time commitment traditional method physically observing joints hand furthermore hand gesture classification vital tool humancomputer interaction gestures used control equipment workplace replace traditional input devices mouse keyboard virtual reality applications lannizzotto et al conn sharma sign language sl commonly learned nonmute people thus mute people problems communicating usually people learn sign language mute person social circle required job engage mute person communication hard tedious example mute individual goes interview interviewer know sl common approach hire translator action creates problems hiring expensive scheduling appointment three people depending circumstances difficult capturing comprehending relationship utterances words critical deaf community order guide us time automatic translation utterances words possible research community long identified need developing sign language technologies facilitate communication social inclusion hearingimpaired people although development technologies really challenging due existence numerous sign languages lack large annotated datasets recent advances al machine learning played significant role automating enhancing technologies hand sign language translation involves translation different sign languages well translation sign speaking languages image recognition techniques play important role automating process identifying signs focus research type problem facilitating communication via sl automating transcription sl without need human translator objectives study general intent study signstospeech mobile app translating sign language speech using convolutional neural network following specific objectives study gather build image dataset american sign language alphabet b train cnn model classify sign language alphabet gathered image dataset c compare performance using adam sgd optimization algorithm performance cnn model develop android application using trained cnn model translate real life sign language letters speech significance study researchers believe results study benefit following deafmutes benefited app could help emotional aspects helps boost confidence chance considered community like normal person app many people possibility learning adopting sign language communication signer interpreter may know speak sign language way would way better would install app like save onthespot situation wherein forgot certain sign language look phone click app know right exact sign language normal people benefit app well makes aware signs allows educated future researchers output research serve input future researchers want conduct study related project next batch researchers upgrade project adding machine learning concepts computer image processing scope delimitations scope scope project build mobile application automatically detect gestures sign language convert text audio without use interpreters delimitations system detect collected hand gesture images az sign languages also hand detection study interested detecting hand gestures collected therefore hand gestures collected identified theoretical framework review related literature section includes ideas generalization conclusions methodologies information relevant related study sandeep kautish asserts study machine learning old technique lately advanced since push use artificial intelligence machine learning improve world wang et al used limit histogram demonstrate turn invariant postures information secured using camera bunching process uses common form computation calculate limit grouping grouped image using skin tone discovery channel multiple networks created image bounds standardized dividing image number sections nin outspread structure according explicit edge harmonys size chain limitwhich used form histogram determined neural networks mlp dynamic programming well dp coordination used classification process despite usage distinct harmonys size histogram distinctive harmonys size fft many analyses conducted various highlight places various studies clearly shown past convolutional neural networks play significant role picture recognition remarkable application track record efficiency dependability cnns unique contribution diagnosis medical disorders tying scanned images presence absence diseases one reliable activation functions used image processing rectified linear unit relu one often used nonlinear activation functions academics rely deep learning projects relu experiments used static positions american sign language work built uniform base purpose acknowledging hand motions stergiopoulou put yet another selfgrowing selforganized neural network song network assessment handformed morphology identified utilizing songarrange shading division approach based skin shading channel ycbcr shading space used hand district identification gaussian circulation model used recognition three highlights extracted using fingerdistinguishing proof process determines quantity lifted fingers quality hand shape according ahmed kasapbasi hearing loss affects worlds population different sign languages created simple effective form communication help people overcome difficulties confront communication sign language relies movements signals convey meaning researchers currently looking ways create sign language recognition systems however many difficulties implementation including inability recognize motions hand poses development recognition systems complicated similarity signs appearances letters foundation languages paper focuses alphabet recognition method used sign language furthermore system described used springboard creating systems complicated imagebased sensorbased sign language identification techniques two main categories first technique depends wearing certain gloves localized sensors techniques key advantage capacity accurately convey information regarding signals gestures including hand movement rotation orientation location second technique makes use several camera types founded image processing call hardware like sensors strategy solely makes use pattern recognition various image processing methods sign languages various nations vary additionally nonmanual signs including body motions facial expressions frequently used different sign languages signs frequently need performed two hands series movements design sign language recognition systems becomes challenging result problems researchers became particularly interested sign language recognition systems means overcoming deep learning recently used researchers improve sign language recognition software order increase systems accuracy numerous approaches datasets used process many factors including regional disparities image types rgb depth others result creation different datasets similar spoken languages sign languages vary one place another include american sign language indian sign language arabic sign language etc additionally whether camera produces rgb depth images determines kind images employed recognition systems additionally fundamental research approaches make various gesture recognition systems differ one system another study strives develop upgrade unique system increase accuracy currently method accurately handle every situation due cnns outstanding performance picture identification researchers previously concentrated cnns various parameters sign language recognition systems classification additionally several research combine cnns techniques provide accurate results others however employed strategies like svm pcanet superiority skill cnns demonstrated comparisons techniques according ying world federation deaf wfd estimates around million deaf persons worldwide utilize sign languages ability read signs could help hearing impaired sighted persons overcome social bariers primary language hearingimpaired people north america american sign language asl utilized name spelling book spelling letter correction since straightforward valuable sign language result asl essential hard hearing unusual circumstances asl always employed supplementary language alphabetic spelling advancement asl recognition system frequently disregarded although majority modem communication tools allow translation spoken written languages insufficient asl therefore developing reliable asl recognition model essential enhance communication serve tool hearingimpaired people help letter correction book spelling name spelling recognition sign language deep leaming approaches employed convolutional neural network cnn typically delivers superior sign language recognition precision among techniques one images always used recognition single stream cnn individual image subjected different convolution kernel operation aid deaf numerous articles understanding sign languages lately published categorize handgrip finger movements adewuyi et al integrated electromyography data fingers arm muscles get higher identification results several works combined several pieces art modal data referred multimodal approach sort neural network used process sequence data recurrent neural network rnn cate et al detected different types vocabulary sign language using rnn time series modeling dualstream rnn network proposed chai et al model could extract gradient histogram characteristics skeleton data use another rnn networks input chalearm gesture recognition challenge model came first place li et al provided new hand type descriptors using lstmbased timing modeling descriptors successfully recognized chinese sign language high accuracy pu et al suggested system understanding sign language based dilated convolutional network threedimensional residual network according ankit ojha gotten harder recent years communicate general public youre deaf implementing system challenging society doesnt competent translator app phones like dream day offering communication app writers made fantastic suggestion deaf community hearing aid community creating app easy operation calls numerous efforts including memory usage implementation totally good design software takes photo sign motion transforms meaningful word order lessen load cpu time first compared gesture using histogram related sample test well brief samples outlined procedure makes simple add gesture software keep database later wider recognition finally made big move deciding employ one app rather number recent customer favorites translator use facial physical expressions postures set gestures humanto human communication well tv social media part sign language according abiyev rahib h millions deaf hearing impaired persons use sign language primary language along hard hearing variety speech challenges approximately persons use sign language form communication according research done british deaf association since practically every nation national sign language fingerspelling alphabet thing worldwide sign language replicate facial expressions also moving lips making manual movements sign languages unique syntax differs significantly spoken languages based speech american sign language asl one widely used sign languages syntax set rules sign language known signed english uses signs asl arranges according english language takes two skills communicate sign language reading signs receptive skills making signs expressive skills integrates hearingimpaired people society promotes equality people translation recognition sign language important research area extremely important issue aimed replacing third human factor creation humanmachine interface improve communication hearingimpaired healthy people translator translation words written fingers text frequently situation recognition sign language alphabet challenge sign languages dynamic combinations variety palm hand gesture positions body movements finally facial expressions rules grammar majority known natural dialects languages various signs figure conceptual framework figure depicts conceptual framework used study first data gathering datasets used development process used two algorithm compare highest accuracy datasets split two sets training validation set used evaluate performance models trained evaluate definition terms asl american sign language cnn convolutional neural network commands specific instruction given computer application perform kind task function dataset collection examples deaf lacking power hearing impaired hearing deep learning type machine learning artificial intelligence imitates way humans gain certain types knowledge epochs number times model work entire dataset finger spelling method spelling words using hand movements gesture movement part body especially hand head express idea meaning grammar way arrange words make proper sentences histogram graph shows frequency numerical data using rectangles image processing performing operations image enhancement extract useful information machine learning science getting computers act without explicitly programmed mobile app computer program software application designed run mobile device phone tablet watch model output training machine learning algorithm mute person speak either inability speak unwillingness speak natural user interface system humancomputer interaction user operates intuitive actions related natural everyday human behavior prediction output trained model applied new data preprocessing processing data used train model sl sign language system communication using visual gestures signs used deaf people translator converts english text american sign language asl used people speech hearing impairment validation process evaluating model training operational framework chapter focuses discussion materials methods utilized describe study conducted first section explains different techniques used training cnn models second section presents different technologies used realization proposed application technologies tools tools used training cnn models throughout process training validating cnn models python programming language used version python created guido van rossum released used python study simplicity stochastic gradient descent sgd optimization algorithm often used machine leaming applications find model parameters correspond best fit predicted actual outputs inexact powerful technique sgd nonconvex iterative firstorder optimization algorithm differentiable error surfaces stochastic estimation gradient descent training data randomized computationally stable mathematically wellestablished optimization algorithm intuition behind sgd take partial derivative objective function respect parameter optimize yields gradient shows increasing direction error loss adam optimization algorithm differs classical stochastic gradient descent algorithmadam popular algorithm field deep leaming achieves good results quickly movement estimation algorithm adam short extension gradient descent natural successor techniques like adagrad rmsprop automatically adapt leaming rate input variable objective function smooth search process using exponentially decreasing moving average gradient make updates variables recently adam optimization algorithm gained lot popularity adam developed diederik p kingma jimmy ba works well place sgd results adam optimizer generally better every optimization algorithm faster computation time requires fewer parameters tuning adam recommended default optimizer applications convolutional neural network cnn subset machine learning one various types artificial neural networks used different applications data types cnn kind network architecture deep learning algorithms specifically used image recognition tasks involve processing pixel data cnns particularly useful finding pattems images recognize objects classes categories cnns first developed used around introduced yann lecun according zhu et al unlike classical models feature extraction part done separately cnns take image data train model automatically classify features cnns composed multiple layers allow model extract important features image used prediction yamashita et al tools used building mobile app react native javascript framework writing real natively rendered mobile applications ios android based react facebooks javascript library building user interfaces instead targeting browser targets mobile platforms words web developers write mobile applications look feel truly native comfort javascript library already know love plus code write shared platforms react native makes easy simultaneously develop android ios react native first released facebook opensource project couple years became one top solutions used mobile development react native development used power worlds leading mobile apps including instagram facebook skype playtorch framework rapidly creating mobile al experiences also known pytorch live helps build alpowered mobile prototypes quickly new release much simpler provides much better developer experience programming language used javascript javascript often abbreviated js programming language one core technologies world wide web one popular languages world javascript invented brendan eich methods dataset creation image datasets used paper asl alphabet dataset images datasets captured camera image data examples shown figure asl gesture language simple expression mainly contains static gestures static gestures gesture represents meaning american letter researchers specified hyperparameters used model training seen table mobile cnn models trained epochs batch size defined stochastic gradient descent sgd adam optimization algorithms used optimizer crossentropy loss defined loss function results discussion chapter presents interprets data gathered study results verification results training mobile cnns validation accuracy two mobile cnns tested validation dataset performance model reported computing accuracy accuracy computed averaging accuracy class dataset precision recall fscore shown tables appendix sign language dataset two mobile cnns showed high performance validation set however models greatly differ time consumed training adam achieved highest accuracy validation dataset trained minutes sgd achieved minutes training time sgd vs adam figure shows training validation accuracy sgd sign language dataset seen accuracy started low loss started result using transfer learning method training time model similar training time transfer training model figure also shows training accuracy loss trained model highest accuracy achieved within epochs training figure sgd accuracy right sgd loss left adam model achieved highest validation accuracy sign language dataset adam achieved accuracy validation set shown figure seen results validation accuracy achieved performed well comparison models also transfer learning method achieved high validation accuracy quickly showing high accuracy even first epoch figure shufflenetv sgd confusion matrix figure shows confusion matrix shufflenetv sgd models sign language dataset confusion matrix shows model got right wrong prediction validation set shufflenetv sgd models correctly labeled images b figure shufflenetv adam confusion matrix figure shows confusion matrix shufflenetv adam model confusion matrix shows shuffleanetv adam models excellent identifying sign language figure main screen app left sign language detection left adam model showed highest validation accuracy task sign language translation sgd showed lowest validation accuracy therefore adam sgdare used building sign language translation app figure shows main screen mobile app one button start detecting hand signs app works first obtaining image used detection image could obtained using phones camera capture image sign language images figures show screenshots mobile apps detection sign language last screenshot image shows incorrect detection sign language detected k actually image v incorrect detection happened insufficient light low quality camera could clearly detect letters alphabet similarity datasets summary conclusions recommendations chapter presents summary research undertaken conclusions drawn recommendations made outgrowth study summary study conducted purpose training handsign language machine learning model use shufflenetv cnn architectures different methodologies procedures used study acquiring image dataset used training models crucial part study image datasets given researchers opensourced researchers developers dataset also verified checking fixing duplicate mislabeled images mobile cnn model used shufflenetv adam model showed highest accuracy validation set sign language sgd showed lowest validation accuracy sign language dataset results showed performance model adam better compared sgd adam achieved highest accuracy sign language dataset models converted torchscript file format allows models run inference mobile phones react native used develop mobile application conclusions based data gathered study following conclusions drawn researchers therefore conclude researchers gathered built image dataset american sign language alphabet used study result researchers conclude trained cnn model classify sign language alphabet collected image dataset fit hand sign language also found using sgd adam optimization algorithm training dataset yields highest validation accuracy adam furthermore developing android application using trained cnn model translate reallife sign language letters speech exemplary lastly ondevice inference makes mobile app widely available users function without use internet connection recommendations results models sign language datasets presented study promising however areas needed improve order make project accurate instrumental based results conclusions presented study following recommendations suggested project would helpful spacing could construct sentence use data highquality images accuracy project would helpful able identify sign language words didnt need letters